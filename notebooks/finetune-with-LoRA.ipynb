{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nurgulkaryagis/miniconda3/envs/rag/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "from peft import get_peft_model, PromptTuningConfig, TaskType, PromptTuningInit\n",
    "from peft import PeftModel, LoraConfig\n",
    "from trl import SFTTrainer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'train_0',\n",
       " 'dialogue': \"#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\\n#Person2#: I found it would be a good idea to get a check-up.\\n#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\\n#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\\n#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\\n#Person2#: Ok.\\n#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\\n#Person2#: Yes.\\n#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\\n#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\\n#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\\n#Person2#: Ok, thanks doctor.\",\n",
       " 'summary': \"Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.\",\n",
       " 'topic': 'get a check-up'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"knkarthick/dialogsum\")\n",
    "dataset_train, dataset_test, dataset_val = dataset[\"train\"], dataset[\"test\"], dataset[\"validation\"]\n",
    "dataset_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"facebook/opt-350m\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize the following:\n",
      " #Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\n",
      "#Person2#: I found it would be a good idea to get a check-up.\n",
      "#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\n",
      "#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\n",
      "#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\n",
      "#Person2#: Ok.\n",
      "#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\n",
      "#Person2#: Yes.\n",
      "#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\n",
      "#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\n",
      "#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\n",
      "#Person2#: Ok, thanks doctor.\n",
      " Summary: Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking. </s> \n"
     ]
    }
   ],
   "source": [
    "def generate_prompt(dialogue, summary=None, eos_token=\"</s>\"):\n",
    "    instruction = \"Summarize the following:\\n\"\n",
    "    input = f\"{dialogue}\\n\"\n",
    "    summary = f\"Summary: {summary + ' ' + eos_token if summary else ''} \"\n",
    "    prompt = (\" \").join([instruction, input, summary])\n",
    "    return prompt\n",
    "\n",
    "print(generate_prompt(dataset_train[0][\"dialogue\"], dataset_train[0][\"summary\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize the following:\n",
      " #Person1#: You have the right to remain silent. Anything you say can and will be used against you in a court of law. You have the right to have an attorney present during questioning. If you cannot afford an attorney, one will be appointed for you. Do you understand?\n",
      "#Person2#: Yes.\n",
      "#Person1#: What's your name?\n",
      "#Person2#: My name is James.\n",
      "#Person1#: What's your nationality?\n",
      "#Person2#: American.\n",
      "#Person1#: What's your relationship with the victim?\n",
      "#Person2#: I don't know him.\n",
      "#Person1#: Why did you attack the victim?\n",
      "#Person2#: Because he beat me first when I tried to stop him from grabbing my bag and running away.\n",
      "#Person1#: How many times did you stab the victim?\n",
      "#Person2#: I stabbed his belly three times.\n",
      "#Person1#: Did you know that your actions might cause serous injuries or death?\n",
      "#Person2#: I knew, but I couldn't control myself.\n",
      "#Person1#: Was it your intention to kill the victim?\n",
      "#Person2#: No. I didn't kill him on purpose, madam. It's him who caused the incident. I need to see my attorney.\n",
      "#Person1#: OK. Give me his number and we'll contact him.\n",
      " Summary:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n"
     ]
    }
   ],
   "source": [
    "input_prompt = generate_prompt(dataset_train[50][\"dialogue\"])\n",
    "input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"]\n",
    "generation_output = model.generate(\n",
    "    input_ids=input_tokens,\n",
    "    max_new_tokens=1000,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    top_p=0.9,\n",
    "    temperature=0.3,\n",
    "    repetition_penalty=1.15,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "  )\n",
    "op = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
    "print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTForCausalLM(\n",
      "  (model): OPTModel(\n",
      "    (decoder): OPTDecoder(\n",
      "      (embed_tokens): Embedding(50272, 512, padding_idx=1)\n",
      "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n",
      "      (project_out): Linear(in_features=1024, out_features=512, bias=False)\n",
      "      (project_in): Linear(in_features=512, out_features=1024, bias=False)\n",
      "      (layers): ModuleList(\n",
      "        (0-23): 24 x OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=512, out_features=50272, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=8,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"],\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50266, 512)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({\"pad_token\": \"<PAD>\"})\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"output\"\n",
    "per_device_train_batch_size = 1 #4\n",
    "gradient_accumulation_steps = 1 #4\n",
    "per_device_eval_batch_size = 2 #4\n",
    "eval_accumulation_steps = 2 #4\n",
    "optim = \"adamw_hf\"\n",
    "save_steps = 10\n",
    "logging_steps = 10\n",
    "learning_rate = 5e-3\n",
    "max_grad_norm = 0.3\n",
    "max_steps = 10\n",
    "warmup_ratio = 0.03\n",
    "evaluation_strategy=\"steps\"\n",
    "lr_scheduler_type = \"constant\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            per_device_train_batch_size=per_device_train_batch_size,\n",
    "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "            optim=optim,\n",
    "            evaluation_strategy=evaluation_strategy,\n",
    "            save_steps=save_steps,\n",
    "            learning_rate=learning_rate,\n",
    "            logging_steps=logging_steps,\n",
    "            max_grad_norm=max_grad_norm,\n",
    "            max_steps=max_steps,\n",
    "            warmup_ratio=warmup_ratio,\n",
    "            group_by_length=True,\n",
    "            lr_scheduler_type=lr_scheduler_type,\n",
    "            ddp_find_unused_parameters=False,\n",
    "            eval_accumulation_steps=eval_accumulation_steps,\n",
    "            per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 19895.00 examples/s]\n",
      "100%|██████████| 10/10 [06:29<00:00, 47.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4789, 'grad_norm': 1.3748090267181396, 'learning_rate': 0.005, 'epoch': 0.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      "100%|██████████| 10/10 [06:33<00:00, 47.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.755002498626709, 'eval_runtime': 3.9947, 'eval_samples_per_second': 0.25, 'eval_steps_per_second': 0.25, 'epoch': 0.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [06:36<00:00, 39.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 396.2925, 'train_samples_per_second': 0.025, 'train_steps_per_second': 0.025, 'train_loss': 2.4789077758789064, 'epoch': 0.77}\n"
     ]
    }
   ],
   "source": [
    "def formatting_func(prompt):\n",
    "    output = []\n",
    "\n",
    "    for d, s in zip(prompt[\"dialogue\"], prompt[\"summary\"]):\n",
    "        op = generate_prompt(d, s)\n",
    "    output.append(op)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_val,\n",
    "    peft_config=lora_config,\n",
    "    formatting_func=formatting_func,\n",
    "    max_seq_length=1024,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args\n",
    ")\n",
    "\n",
    "# We will also pre-process the model by upcasting the layer norms in float 32 for more stable training\n",
    "for name, module in trainer.model.named_modules():\n",
    "    if \"norm\" in name:\n",
    "        module = module.to(torch.float32)\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(f\"{output_dir}/final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model_id = \"output/checkpoint-10\"\n",
    "peft_model = PeftModel.from_pretrained(model, peft_model_id, torch_dtype=torch.float16, offload_folder=\"lora_results/lora_7/temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompt = generate_prompt(dataset_train[50][\"dialogue\"])\n",
    "input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"]\n",
    "generation_output = peft_model.generate(\n",
    "    input_ids=input_tokens,\n",
    "    max_new_tokens=100,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    top_p=0.9,\n",
    "    temperature=0.3,\n",
    "    repetition_penalty=1.15,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "  )\n",
    "op = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
    "print(op)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
